{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6f479a",
   "metadata": {},
   "source": [
    "load env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511115e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"PINECODE_API_KEY\"]=os.getenv(\"PINECODE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"]=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=os.getenv(\"LANGCHAIN_TRACING_V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261355be",
   "metadata": {},
   "source": [
    "fetch the data from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Load PDF document\n",
    "pdf_loader = PyPDFLoader(\n",
    "    file_path = \"/Users/midas-ai/Documents/Personal/agentic-ai-2.0-course/workspace/2-Langchain- Basics/Assignment/attention-all-you-need.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d70efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages=pdf_loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8d9c5",
   "metadata": {},
   "source": [
    "chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "139e9b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a semantic chunker\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "\n",
    "# Split documents semantically\n",
    "chunked_docs = text_splitter.split_documents(pages)\n",
    "len(chunked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7d5e",
   "metadata": {},
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f21585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r6/94rtxyj11sgb8st7ytmz40ph0000gn/T/ipykernel_85897/1523018376.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings=OpenAIEmbeddings( model=\"text-embedding-3-large\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings( model=\"text-embedding-3-large\")\n",
    "\n",
    "len(embeddings.embed_query(\"hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa1215",
   "metadata": {},
   "source": [
    "saving documents in vector store [milvus-local]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e88710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 20:33:18,519 [DEBUG][_create_connection]: Created new connection using: 8123d3be1ac1458bacd2c2eebfb5ca36 (async_milvus_client.py:599)\n"
     ]
    }
   ],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "URI = \"./milvus_local.db\"\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\":URI},\n",
    "    index_params={\"index_type\":\"FLAT\", \"metric_type\":\"L2\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c92e0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "#universal indentification number\n",
    "uuids = [str(uuid4()) for _ in range(len(chunked_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eacfaeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[458570899064094720,\n",
       " 458570899064094721,\n",
       " 458570899064094722,\n",
       " 458570899064094723,\n",
       " 458570899064094724,\n",
       " 458570899064094725,\n",
       " 458570899064094726,\n",
       " 458570899064094727,\n",
       " 458570899064094728,\n",
       " 458570899064094729,\n",
       " 458570899064094730,\n",
       " 458570899064094731,\n",
       " 458570899064094732,\n",
       " 458570899064094733,\n",
       " 458570899064094734,\n",
       " 458570899064094735,\n",
       " 458570899064094736,\n",
       " 458570899064094737,\n",
       " 458570899064094738,\n",
       " 458570899064094739,\n",
       " 458570899064094740,\n",
       " 458570899064094741,\n",
       " 458570899064094742,\n",
       " 458570899064094743,\n",
       " 458570899064094744,\n",
       " 458570899064094745,\n",
       " 458570899064094746,\n",
       " 458570899064094747,\n",
       " 458570899064094748,\n",
       " 458570899064094749,\n",
       " 458570899064094750,\n",
       " 458570899064094751,\n",
       " 458570899064094752,\n",
       " 458570899064094753,\n",
       " 458570899064094754,\n",
       " 458570899064094755,\n",
       " 458570899064094756,\n",
       " 458570899064094757,\n",
       " 458570899064094758,\n",
       " 458570899064094759,\n",
       " 458570899064094760,\n",
       " 458570899064094761,\n",
       " 458570899064094762,\n",
       " 458570899064094763,\n",
       " 458570899064094764,\n",
       " 458570899064094765,\n",
       " 458570899064094766,\n",
       " 458570899064094767,\n",
       " 458570899064094768,\n",
       " 458570899064094769,\n",
       " 458570899064094770,\n",
       " 458570899064094771,\n",
       " 458570899064094772,\n",
       " 458570899064094773,\n",
       " 458570899064094774,\n",
       " 458570899064094775,\n",
       " 458570899064094776,\n",
       " 458570899064094777,\n",
       " 458570899064094778,\n",
       " 458570899064094779,\n",
       " 458570899064094780,\n",
       " 458570899064094781,\n",
       " 458570899064094782,\n",
       " 458570899064094783,\n",
       " 458570899064094784,\n",
       " 458570899064094785,\n",
       " 458570899064094786,\n",
       " 458570899064094787,\n",
       " 458570899064094788,\n",
       " 458570899064094789,\n",
       " 458570899064094790,\n",
       " 458570899064094791,\n",
       " 458570899064094792,\n",
       " 458570899064094793,\n",
       " 458570899064094794,\n",
       " 458570899064094795,\n",
       " 458570899064094796,\n",
       " 458570899064094797,\n",
       " 458570899064094798,\n",
       " 458570899064094799,\n",
       " 458570899064094800,\n",
       " 458570899064094801,\n",
       " 458570899064094802,\n",
       " 458570899064094803,\n",
       " 458570899064094804,\n",
       " 458570899064094805,\n",
       " 458570899064094806,\n",
       " 458570899064094807,\n",
       " 458570899064094808,\n",
       " 458570899064094809,\n",
       " 458570899064094810,\n",
       " 458570899064094811,\n",
       " 458570899064094812]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_metadata_keys(doc):\n",
    "    new_metadata = {}\n",
    "    for key, value in doc.metadata.items():\n",
    "        # Replace invalid characters with underscore\n",
    "        sanitized_key = re.sub(r'\\W|^(?=\\d)', '_', key)\n",
    "        new_metadata[sanitized_key] = value\n",
    "    doc.metadata = new_metadata\n",
    "    return doc\n",
    "\n",
    "# Apply to all documents\n",
    "chunked_docs = [sanitize_metadata_keys(doc) for doc in chunked_docs]\n",
    "\n",
    "vector_store.add_documents(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4788418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to retrieve documents: 0.90 seconds\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Operation to measure\n",
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.1} #hyperparameter\n",
    ")\n",
    "results=retriever.invoke(\"self-attention mechanism\")\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "# Print duration\n",
    "print(f\"Time taken to retrieve documents: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e2f5e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Document Content: reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "Similarity Score: 0.6449\n",
      "\n",
      "Result 2:\n",
      "Document Content: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "Similarity Score: 0.7812\n",
      "\n",
      "Result 3:\n",
      "Document Content: (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "Similarity Score: 0.8027\n",
      "\n",
      "Result 4:\n",
      "Document Content: masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "Similarity Score: 0.8172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"self-attention mechanism\"\n",
    "results_with_scores = vector_store.similarity_search_with_score(query)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_scores):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Document Content: {doc.page_content}\")\n",
    "    print(f\"Similarity Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19665d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 1:\n",
      "reduced to a constan\n",
      "\n",
      "Initial 2:\n",
      "• In \"encoder-decode\n",
      "\n",
      "Initial 3:\n",
      "(x1, ..., xn) to ano\n",
      "\n",
      "Rank 1:\n",
      "reduced to a constan\n",
      "\n",
      "Rank 2:\n",
      "• In \"encoder-decode\n",
      "\n",
      "Rank 3:\n",
      "(x1, ..., xn) to ano\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Vector similarity search\n",
    "query = \"self-attention mechanism\"\n",
    "top_k = 3\n",
    "initial_results = vector_store.similarity_search(query, k=top_k)\n",
    "\n",
    "for i, doc in enumerate(initial_results):\n",
    "    print(f\"Initial {i+1}:\")\n",
    "    print(doc.page_content[:20])\n",
    "    print()\n",
    "\n",
    "# Step 2: Prepare documents for BM25\n",
    "bm25_retriever = BM25Retriever.from_documents(initial_results)\n",
    "\n",
    "# Step 3: Perform reranking using BM25\n",
    "reranked_docs = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Step 4: Print results\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(doc.page_content[:20])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab39f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt=hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "572937cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model=ChatGoogleGenerativeAI(model='gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5f20f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    { \"context\": retriever | format_docs , \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10d19da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response= rag_chain.invoke(\"What is Transformers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14926942",
   "metadata": {},
   "source": [
    "saving the llm output in docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8e77936",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Document.__init__() missing 1 required positional argument: 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 3: Write the LLM output to a new .docx file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m output_doc = \u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m output_doc.add_heading(\u001b[33m\"\u001b[39m\u001b[33mLLM Output\u001b[39m\u001b[33m\"\u001b[39m, level=\u001b[32m1\u001b[39m)\n\u001b[32m      4\u001b[39m output_doc.add_paragraph(response)\n",
      "\u001b[31mTypeError\u001b[39m: Document.__init__() missing 1 required positional argument: 'page_content'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Write the LLM output to a new .docx file\n",
    "output_doc = Document()\n",
    "output_doc.add_heading(\"LLM Output\", level=1)\n",
    "output_doc.add_paragraph(response)\n",
    "output_doc.save(\"./output.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398cf427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
